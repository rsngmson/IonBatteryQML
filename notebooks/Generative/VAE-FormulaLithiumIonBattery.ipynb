{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5758f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fa0d1638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 25\n",
      "Max sequence length: 15\n",
      "Sample tokenized formula: Li 2 Mn Si O 4\n",
      "Sample sequence: [2, 5, 9, 3, 4, 6]\n",
      "Prepared 339 sequences\n",
      "\n",
      "Model built successfully!\n",
      "Encoder summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"encoder\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"encoder\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_14        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,200</span> │ encoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_14        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ encoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_54 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> │ embedding_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │                   │            │ not_equal_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_55 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">197,120</span> │ lstm_54[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│                     │                   │            │ not_equal_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_39 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │ lstm_55[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ z_mean (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,128</span> │ dense_39[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ z_log_var (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,128</span> │ dense_39[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_14        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │      \u001b[38;5;34m3,200\u001b[0m │ encoder_input[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_14        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ encoder_input[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_54 (\u001b[38;5;33mLSTM\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │    \u001b[38;5;34m394,240\u001b[0m │ embedding_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │                   │            │ not_equal_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_55 (\u001b[38;5;33mLSTM\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │    \u001b[38;5;34m197,120\u001b[0m │ lstm_54[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
       "│                     │                   │            │ not_equal_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_39 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m16,512\u001b[0m │ lstm_55[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ z_mean (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │      \u001b[38;5;34m4,128\u001b[0m │ dense_39[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ z_log_var (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │      \u001b[38;5;34m4,128\u001b[0m │ dense_39[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">619,328</span> (2.36 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m619,328\u001b[0m (2.36 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">619,328</span> (2.36 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m619,328\u001b[0m (2.36 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decoder summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"decoder\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"decoder\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ z_sampling (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_40 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,224</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_41 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ repeat_vector_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RepeatVector</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_56 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_57 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">197,120</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,225</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ z_sampling (\u001b[38;5;33mInputLayer\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_40 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m4,224\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_41 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m33,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ repeat_vector_13 (\u001b[38;5;33mRepeatVector\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_56 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │       \u001b[38;5;34m525,312\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_57 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │       \u001b[38;5;34m197,120\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_output (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m25\u001b[0m)         │         \u001b[38;5;34m3,225\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">762,905</span> (2.91 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m762,905\u001b[0m (2.91 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">762,905</span> (2.91 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m762,905\u001b[0m (2.91 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model...\n",
      "Starting training...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[32m/var/folders/7m/q4tfxjb53l137r3zt2w5xgb40000gn/T/ipykernel_58169/3937901343.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    344\u001b[39m     vae.decoder.summary()\n\u001b[32m    345\u001b[39m \n\u001b[32m    346\u001b[39m     \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m    347\u001b[39m     print(\u001b[33m\"\\nTraining model...\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m348\u001b[39m     history = vae.train(sequences, epochs=\u001b[32m50\u001b[39m, batch_size=\u001b[32m8\u001b[39m)\n\u001b[32m    349\u001b[39m \n\u001b[32m    350\u001b[39m     \u001b[38;5;66;03m# Generate công thức mới\u001b[39;00m\n\u001b[32m    351\u001b[39m     print(\u001b[33m\"\\nGenerating new formulas:\"\u001b[39m)\n",
      "\u001b[32m/var/folders/7m/q4tfxjb53l137r3zt2w5xgb40000gn/T/ipykernel_58169/3937901343.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, sequences, epochs, batch_size)\u001b[39m\n\u001b[32m    199\u001b[39m                 \u001b[33m'kl_loss'\u001b[39m: []\n\u001b[32m    200\u001b[39m             }\n\u001b[32m    201\u001b[39m \n\u001b[32m    202\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;28;01min\u001b[39;00m dataset:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m                 losses = self.train_step(batch)\n\u001b[32m    204\u001b[39m                 \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;28;01min\u001b[39;00m epoch_losses:\n\u001b[32m    205\u001b[39m                     epoch_losses[key].append(losses[key])\n\u001b[32m    206\u001b[39m \n",
      "\u001b[32m/var/folders/7m/q4tfxjb53l137r3zt2w5xgb40000gn/T/ipykernel_58169/3937901343.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, x_batch)\u001b[39m\n\u001b[32m    166\u001b[39m         gradients = tape.gradient(total_loss, trainable_vars)\n\u001b[32m    167\u001b[39m \n\u001b[32m    168\u001b[39m         \u001b[38;5;66;03m# Apply gradients\u001b[39;00m\n\u001b[32m    169\u001b[39m         optimizer = Adam(learning_rate=\u001b[32m0.001\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m         optimizer.apply_gradients(zip(gradients, trainable_vars))\n\u001b[32m    171\u001b[39m \n\u001b[32m    172\u001b[39m         return {\n\u001b[32m    173\u001b[39m             \u001b[33m'total_loss'\u001b[39m: total_loss,\n",
      "\u001b[32m~/anaconda3/envs/WSAIPytorch3.11/lib/python3.11/site-packages/keras/src/optimizers/base_optimizer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, grads_and_vars)\u001b[39m\n\u001b[32m    381\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m apply_gradients(self, grads_and_vars):\n\u001b[32m    382\u001b[39m         grads, trainable_variables = zip(*grads_and_vars)\n\u001b[32m--> \u001b[39m\u001b[32m383\u001b[39m         self.apply(grads, trainable_variables)\n\u001b[32m    384\u001b[39m         \u001b[38;5;66;03m# Return iterations for compat with tf.keras.\u001b[39;00m\n\u001b[32m    385\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m self._iterations\n",
      "\u001b[32m~/anaconda3/envs/WSAIPytorch3.11/lib/python3.11/site-packages/keras/src/optimizers/base_optimizer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, grads, trainable_variables)\u001b[39m\n\u001b[32m    444\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m scale \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    445\u001b[39m                 grads = [g \u001b[38;5;28;01mif\u001b[39;00m g \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m g / scale \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;28;01min\u001b[39;00m grads]\n\u001b[32m    446\u001b[39m \n\u001b[32m    447\u001b[39m             \u001b[38;5;66;03m# Apply gradient updates.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m448\u001b[39m             self._backend_apply_gradients(grads, trainable_variables)\n\u001b[32m    449\u001b[39m             \u001b[38;5;66;03m# Apply variable constraints after applying gradients.\u001b[39;00m\n\u001b[32m    450\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m variable \u001b[38;5;28;01min\u001b[39;00m trainable_variables:\n\u001b[32m    451\u001b[39m                 \u001b[38;5;28;01mif\u001b[39;00m variable.constraint \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m~/anaconda3/envs/WSAIPytorch3.11/lib/python3.11/site-packages/keras/src/optimizers/base_optimizer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, grads, trainable_variables)\u001b[39m\n\u001b[32m    507\u001b[39m             grads = self._clip_gradients(grads)\n\u001b[32m    508\u001b[39m             self._apply_weight_decay(trainable_variables)\n\u001b[32m    509\u001b[39m \n\u001b[32m    510\u001b[39m             \u001b[38;5;66;03m# Run update step.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m             self._backend_update_step(\n\u001b[32m    512\u001b[39m                 grads, trainable_variables, self.learning_rate\n\u001b[32m    513\u001b[39m             )\n\u001b[32m    514\u001b[39m \n",
      "\u001b[32m~/anaconda3/envs/WSAIPytorch3.11/lib/python3.11/site-packages/keras/src/backend/tensorflow/optimizer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, grads, trainable_variables, learning_rate)\u001b[39m\n\u001b[32m    116\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;28;01min\u001b[39;00m trainable_variables\n\u001b[32m    117\u001b[39m         ]\n\u001b[32m    118\u001b[39m         grads_and_vars = list(zip(grads, trainable_variables))\n\u001b[32m    119\u001b[39m         grads_and_vars = self._all_reduce_sum_gradients(grads_and_vars)\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         tf.__internal__.distribute.interim.maybe_merge_call(\n\u001b[32m    121\u001b[39m             self._distributed_tf_update_step,\n\u001b[32m    122\u001b[39m             self._distribution_strategy,\n\u001b[32m    123\u001b[39m             grads_and_vars,\n",
      "\u001b[32m~/anaconda3/envs/WSAIPytorch3.11/lib/python3.11/site-packages/tensorflow/python/distribute/merge_call_interim.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(fn, strategy, *args, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m   Returns:\n\u001b[32m     48\u001b[39m     The \u001b[38;5;28;01mreturn\u001b[39;00m value of the `fn` call.\n\u001b[32m     49\u001b[39m   \"\"\"\n\u001b[32m     50\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m strategy_supports_no_merge_call():\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(strategy, *args, **kwargs)\n\u001b[32m     52\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     53\u001b[39m     return distribute_lib.get_replica_context().merge_call(\n\u001b[32m     54\u001b[39m         fn, args=args, kwargs=kwargs)\n",
      "\u001b[32m~/anaconda3/envs/WSAIPytorch3.11/lib/python3.11/site-packages/keras/src/backend/tensorflow/optimizer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, distribution, grads_and_vars, learning_rate)\u001b[39m\n\u001b[32m    130\u001b[39m         \u001b[38;5;28;01mdef\u001b[39;00m apply_grad_to_update_var(var, grad, learning_rate):\n\u001b[32m    131\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self.update_step(grad, var, learning_rate)\n\u001b[32m    132\u001b[39m \n\u001b[32m    133\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m grad, var \u001b[38;5;28;01min\u001b[39;00m grads_and_vars:\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m             distribution.extended.update(\n\u001b[32m    135\u001b[39m                 var,\n\u001b[32m    136\u001b[39m                 apply_grad_to_update_var,\n\u001b[32m    137\u001b[39m                 args=(grad, learning_rate),\n",
      "\u001b[32m~/anaconda3/envs/WSAIPytorch3.11/lib/python3.11/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, var, fn, args, kwargs, group)\u001b[39m\n\u001b[32m   3001\u001b[39m         _get_default_replica_context()):\n\u001b[32m   3002\u001b[39m       fn = autograph.tf_convert(\n\u001b[32m   3003\u001b[39m           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[32m   3004\u001b[39m       \u001b[38;5;28;01mwith\u001b[39;00m self._container_strategy().scope():\n\u001b[32m-> \u001b[39m\u001b[32m3005\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m self._update(var, fn, args, kwargs, group)\n\u001b[32m   3006\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3007\u001b[39m       return self._replica_ctx_update(\n\u001b[32m   3008\u001b[39m           var, fn, args=args, kwargs=kwargs, group=group)\n",
      "\u001b[32m~/anaconda3/envs/WSAIPytorch3.11/lib/python3.11/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, var, fn, args, kwargs, group)\u001b[39m\n\u001b[32m   4072\u001b[39m   \u001b[38;5;28;01mdef\u001b[39;00m _update(self, var, fn, args, kwargs, group):\n\u001b[32m   4073\u001b[39m     \u001b[38;5;66;03m# The implementations of _update() and _update_non_slot() are identical\u001b[39;00m\n\u001b[32m   4074\u001b[39m     \u001b[38;5;66;03m# except _update() passes `var` as the first argument to `fn()`.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4075\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m self._update_non_slot(var, fn, (var,) + tuple(args), kwargs, group)\n",
      "\u001b[32m~/anaconda3/envs/WSAIPytorch3.11/lib/python3.11/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[39m\n\u001b[32m   4077\u001b[39m   \u001b[38;5;28;01mdef\u001b[39;00m _update_non_slot(self, colocate_with, fn, args, kwargs, should_group):\n\u001b[32m   4078\u001b[39m     \u001b[38;5;66;03m# TODO(josh11b): Figure out what we should be passing to UpdateContext()\u001b[39;00m\n\u001b[32m   4079\u001b[39m     \u001b[38;5;66;03m# once that value is used for something.\u001b[39;00m\n\u001b[32m   4080\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m UpdateContext(colocate_with):\n\u001b[32m-> \u001b[39m\u001b[32m4081\u001b[39m       result = fn(*args, **kwargs)\n\u001b[32m   4082\u001b[39m       \u001b[38;5;28;01mif\u001b[39;00m should_group:\n\u001b[32m   4083\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m   4084\u001b[39m       \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m~/anaconda3/envs/WSAIPytorch3.11/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    594\u001b[39m   \u001b[38;5;28;01mdef\u001b[39;00m wrapper(*args, **kwargs):\n\u001b[32m    595\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx.ControlStatusCtx(status=ag_ctx.Status.UNSPECIFIED):\n\u001b[32m--> \u001b[39m\u001b[32m596\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[32m~/anaconda3/envs/WSAIPytorch3.11/lib/python3.11/site-packages/keras/src/backend/tensorflow/optimizer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(var, grad, learning_rate)\u001b[39m\n\u001b[32m    130\u001b[39m         \u001b[38;5;28;01mdef\u001b[39;00m apply_grad_to_update_var(var, grad, learning_rate):\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self.update_step(grad, var, learning_rate)\n",
      "\u001b[32m~/anaconda3/envs/WSAIPytorch3.11/lib/python3.11/site-packages/keras/src/optimizers/adam.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, gradient, variable, learning_rate)\u001b[39m\n\u001b[32m    134\u001b[39m         )\n\u001b[32m    135\u001b[39m         self.assign_add(\n\u001b[32m    136\u001b[39m             v,\n\u001b[32m    137\u001b[39m             ops.multiply(\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m                 ops.subtract(ops.square(gradient), v), \u001b[32m1\u001b[39m - self.beta_2\n\u001b[32m    139\u001b[39m             ),\n\u001b[32m    140\u001b[39m         )\n\u001b[32m    141\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m self.amsgrad:\n",
      "\u001b[32m~/anaconda3/envs/WSAIPytorch3.11/lib/python3.11/site-packages/keras/src/ops/numpy.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(x1, x2)\u001b[39m\n\u001b[32m   6074\u001b[39m         Output tensor, element-wise difference of `x1` \u001b[38;5;28;01mand\u001b[39;00m `x2`.\n\u001b[32m   6075\u001b[39m     \"\"\"\n\u001b[32m   6076\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors((x1, x2)):\n\u001b[32m   6077\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m Subtract().symbolic_call(x1, x2)\n\u001b[32m-> \u001b[39m\u001b[32m6078\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m backend.numpy.subtract(x1, x2)\n",
      "\u001b[32m~/anaconda3/envs/WSAIPytorch3.11/lib/python3.11/site-packages/keras/src/backend/tensorflow/sparse.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(x1, x2)\u001b[39m\n\u001b[32m    489\u001b[39m                     x1 = tf.convert_to_tensor(x1)\n\u001b[32m    490\u001b[39m             \u001b[38;5;28;01melif\u001b[39;00m isinstance(x2, tf.IndexedSlices):\n\u001b[32m    491\u001b[39m                 \u001b[38;5;66;03m# x2 is an IndexedSlices, densify.\u001b[39;00m\n\u001b[32m    492\u001b[39m                 x2 = tf.convert_to_tensor(x2)\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m func(x1, x2)\n",
      "\u001b[32m~/anaconda3/envs/WSAIPytorch3.11/lib/python3.11/site-packages/keras/src/backend/tensorflow/numpy.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(x1, x2)\u001b[39m\n\u001b[32m    435\u001b[39m         getattr(x2, \u001b[33m\"dtype\"\u001b[39m, type(x2)),\n\u001b[32m    436\u001b[39m     )\n\u001b[32m    437\u001b[39m     x1 = convert_to_tensor(x1, dtype)\n\u001b[32m    438\u001b[39m     x2 = convert_to_tensor(x2, dtype)\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tf.subtract(x1, x2)\n",
      "\u001b[32m~/anaconda3/envs/WSAIPytorch3.11/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Exception \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m       filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    153\u001b[39m       \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m       \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[32m~/anaconda3/envs/WSAIPytorch3.11/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1257\u001b[39m \n\u001b[32m   1258\u001b[39m       \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[32m   1259\u001b[39m       \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1260\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(*args, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1261\u001b[39m       \u001b[38;5;28;01mexcept\u001b[39;00m (TypeError, ValueError):\n\u001b[32m   1262\u001b[39m         \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[32m   1263\u001b[39m         \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[32m   1264\u001b[39m         result = dispatch(op_dispatch_handler, args, kwargs)\n",
      "\u001b[32m~/anaconda3/envs/WSAIPytorch3.11/lib/python3.11/site-packages/tensorflow/python/ops/math_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(x, y, name)\u001b[39m\n\u001b[32m    541\u001b[39m @tf_export(\u001b[33m\"math.subtract\"\u001b[39m, \u001b[33m\"subtract\"\u001b[39m)\n\u001b[32m    542\u001b[39m @dispatch.register_binary_elementwise_api\n\u001b[32m    543\u001b[39m @dispatch.add_dispatch_support\n\u001b[32m    544\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m subtract(x, y, name=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m545\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m gen_math_ops.sub(x, y, name)\n",
      "\u001b[32m~/anaconda3/envs/WSAIPytorch3.11/lib/python3.11/site-packages/tensorflow/python/ops/weak_tensor_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    140\u001b[39m   \u001b[38;5;28;01mdef\u001b[39;00m wrapper(*args, **kwargs):\n\u001b[32m    141\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m ops.is_auto_dtype_conversion_enabled():\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m op(*args, **kwargs)\n\u001b[32m    143\u001b[39m     bound_arguments = signature.bind(*args, **kwargs)\n\u001b[32m    144\u001b[39m     bound_arguments.apply_defaults()\n\u001b[32m    145\u001b[39m     bound_kwargs = bound_arguments.arguments\n",
      "\u001b[32m~/anaconda3/envs/WSAIPytorch3.11/lib/python3.11/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(x, y, name)\u001b[39m\n\u001b[32m  12309\u001b[39m       \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m  12310\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m  12311\u001b[39m       return sub_eager_fallback(\n\u001b[32m  12312\u001b[39m           x, y, name=name, ctx=_ctx)\n\u001b[32m> \u001b[39m\u001b[32m12313\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m _core._SymbolicException:\n\u001b[32m  12314\u001b[39m       \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n\u001b[32m  12315\u001b[39m   \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n\u001b[32m  12316\u001b[39m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n",
      "\u001b[32m~/anaconda3/envs/WSAIPytorch3.11/lib/python3.11/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(x, y, name, ctx)\u001b[39m\n\u001b[32m> \u001b[39m\u001b[32m12330\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m sub_eager_fallback(x: Annotated[Any, TV_Sub_T], y: Annotated[Any, TV_Sub_T], name, ctx) -> Annotated[Any, TV_Sub_T]:\n\u001b[32m  12331\u001b[39m   _attr_T, _inputs_T = _execute.args_to_matching_eager([x, y], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, _dtypes.uint8, _dtypes.int8, _dtypes.uint16, _dtypes.int16, _dtypes.int32, _dtypes.int64, _dtypes.complex64, _dtypes.complex128, _dtypes.uint32, _dtypes.uint64, ])\n\u001b[32m  12332\u001b[39m   (x, y) = _inputs_T\n\u001b[32m  12333\u001b[39m   _inputs_flat = [x, y]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "class ChemicalFormulaVAE:\n",
    "    def __init__(self, latent_dim=64, max_length=20):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = None\n",
    "        self.vocab_size = None\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        \n",
    "    def tokenize_formula(self, formula):\n",
    "        \"\"\"\n",
    "        Tokenize công thức hóa học thành các token\n",
    "        Ví dụ: 'Li2MnSiO4' -> ['Li', '2', 'Mn', 'Si', 'O', '4']\n",
    "        \"\"\"\n",
    "        # Regex để tách element và số\n",
    "        pattern = r'([A-Z][a-z]*)(\\d*)'\n",
    "        tokens = []\n",
    "        \n",
    "        matches = re.findall(pattern, formula)\n",
    "        for element, number in matches:\n",
    "            tokens.append(element)\n",
    "            if number:\n",
    "                tokens.append(number)\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def prepare_data(self, formulas):\n",
    "        \"\"\"\n",
    "        Chuẩn bị dữ liệu cho training\n",
    "        \"\"\"\n",
    "        # Tokenize tất cả công thức\n",
    "        tokenized_formulas = []\n",
    "        for formula in formulas:\n",
    "            tokens = self.tokenize_formula(formula)\n",
    "            tokenized_formulas.append(' '.join(tokens))\n",
    "        \n",
    "        # Tạo tokenizer\n",
    "        # self.tokenizer = Tokenizer(char_level=False)\n",
    "        self.tokenizer = Tokenizer(char_level=False, lower=False, oov_token='<OOV>') # Thêm để tránh các token thành lowcase hết\n",
    "        self.tokenizer.fit_on_texts(tokenized_formulas)\n",
    "        self.vocab_size = len(self.tokenizer.word_index) + 1\n",
    "        \n",
    "        # Chuyển đổi thành sequences\n",
    "        sequences = self.tokenizer.texts_to_sequences(tokenized_formulas)\n",
    "        \n",
    "        # Padding\n",
    "        padded_sequences = pad_sequences(sequences, maxlen=self.max_length, padding='post')\n",
    "        \n",
    "        print(f\"Vocab size: {self.vocab_size}\")\n",
    "        print(f\"Max sequence length: {self.max_length}\")\n",
    "        print(f\"Sample tokenized formula: {tokenized_formulas[0]}\")\n",
    "        print(f\"Sample sequence: {sequences[0]}\")\n",
    "        \n",
    "        return padded_sequences\n",
    "    \n",
    "    def build_encoder(self):\n",
    "        \"\"\"\n",
    "        Xây dựng encoder\n",
    "        \"\"\"\n",
    "        encoder_inputs = Input(shape=(self.max_length,), name='encoder_input')\n",
    "        \n",
    "        # Embedding layer\n",
    "        x = Embedding(self.vocab_size, 128, mask_zero=True)(encoder_inputs)\n",
    "        \n",
    "        # LSTM layers\n",
    "        x = LSTM(256, return_sequences=True, dropout=0.2)(x)\n",
    "        x = LSTM(128, dropout=0.2)(x)\n",
    "        \n",
    "        # Dense layers\n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        \n",
    "        # Latent space\n",
    "        z_mean = Dense(self.latent_dim, name='z_mean')(x)\n",
    "        z_log_var = Dense(self.latent_dim, name='z_log_var')(x)\n",
    "        \n",
    "        encoder = Model(encoder_inputs, [z_mean, z_log_var], name='encoder')\n",
    "        return encoder\n",
    "    \n",
    "    def build_decoder(self):\n",
    "        \"\"\"\n",
    "        Xây dựng decoder\n",
    "        \"\"\"\n",
    "        latent_inputs = Input(shape=(self.latent_dim,), name='z_sampling')\n",
    "        \n",
    "        # Dense layers\n",
    "        x = Dense(128, activation='relu')(latent_inputs)\n",
    "        x = Dense(256, activation='relu')(x)\n",
    "        \n",
    "        # Repeat for sequence generation\n",
    "        x = RepeatVector(self.max_length)(x)\n",
    "        \n",
    "        # LSTM layers\n",
    "        x = LSTM(256, return_sequences=True, dropout=0.2)(x)\n",
    "        x = LSTM(128, return_sequences=True, dropout=0.2)(x)\n",
    "        \n",
    "        # Output layer\n",
    "        decoder_outputs = Dense(self.vocab_size, activation='softmax', name='decoder_output')(x)\n",
    "        \n",
    "        decoder = Model(latent_inputs, decoder_outputs, name='decoder')\n",
    "        return decoder\n",
    "    \n",
    "    def sampling(self, z_mean, z_log_var):\n",
    "        \"\"\"\n",
    "        Reparameterization trick\n",
    "        \"\"\"\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.random.normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "    \n",
    "    def build_models(self):\n",
    "        \"\"\"\n",
    "        Xây dựng encoder và decoder\n",
    "        \"\"\"\n",
    "        self.encoder = self.build_encoder()\n",
    "        self.decoder = self.build_decoder()\n",
    "        \n",
    "        # Compile models\n",
    "        self.encoder.compile(optimizer=Adam(learning_rate=0.001))\n",
    "        self.decoder.compile(optimizer=Adam(learning_rate=0.001), \n",
    "                           loss='sparse_categorical_crossentropy',\n",
    "                           metrics=['accuracy'])\n",
    "    \n",
    "    def train_step(self, x_batch):\n",
    "        \"\"\"\n",
    "        Custom training step\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Encode\n",
    "            z_mean, z_log_var = self.encoder(x_batch)\n",
    "            \n",
    "            # Sample\n",
    "            z = self.sampling(z_mean, z_log_var)\n",
    "            \n",
    "            # Decode\n",
    "            reconstruction = self.decoder(z)\n",
    "            \n",
    "            # Reconstruction loss\n",
    "            reconstruction_loss = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "                x_batch, reconstruction\n",
    "            )\n",
    "            reconstruction_loss = tf.reduce_mean(reconstruction_loss)\n",
    "            \n",
    "            # KL divergence loss\n",
    "            kl_loss = -0.5 * tf.reduce_mean(\n",
    "                1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
    "            )\n",
    "            \n",
    "            # Total loss\n",
    "            total_loss = reconstruction_loss + 0.1 * kl_loss\n",
    "        \n",
    "        # Get trainable variables from both encoder and decoder\n",
    "        trainable_vars = self.encoder.trainable_variables + self.decoder.trainable_variables\n",
    "        \n",
    "        # Compute gradients\n",
    "        gradients = tape.gradient(total_loss, trainable_vars)\n",
    "        \n",
    "        # Apply gradients\n",
    "        optimizer = Adam(learning_rate=0.001)\n",
    "        optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        \n",
    "        return {\n",
    "            'total_loss': total_loss,\n",
    "            'reconstruction_loss': reconstruction_loss,\n",
    "            'kl_loss': kl_loss\n",
    "        }\n",
    "    \n",
    "    def train(self, sequences, epochs=100, batch_size=32):\n",
    "        \"\"\"\n",
    "        Huấn luyện VAE với custom training loop\n",
    "        \"\"\"\n",
    "        if self.encoder is None or self.decoder is None:\n",
    "            self.build_models()\n",
    "        \n",
    "        dataset = tf.data.Dataset.from_tensor_slices(sequences)\n",
    "        dataset = dataset.batch(batch_size).shuffle(1000)\n",
    "        \n",
    "        history = {\n",
    "            'total_loss': [],\n",
    "            'reconstruction_loss': [],\n",
    "            'kl_loss': []\n",
    "        }\n",
    "        \n",
    "        print(\"Starting training...\")\n",
    "        for epoch in range(epochs):\n",
    "            epoch_losses = {\n",
    "                'total_loss': [],\n",
    "                'reconstruction_loss': [],\n",
    "                'kl_loss': []\n",
    "            }\n",
    "            \n",
    "            for batch in dataset:\n",
    "                losses = self.train_step(batch)\n",
    "                for key in epoch_losses:\n",
    "                    epoch_losses[key].append(losses[key])\n",
    "            \n",
    "            # Calculate average losses for this epoch\n",
    "            avg_losses = {}\n",
    "            for key in epoch_losses:\n",
    "                avg_losses[key] = tf.reduce_mean(epoch_losses[key])\n",
    "                history[key].append(float(avg_losses[key]))\n",
    "            \n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs} - \"\n",
    "                      f\"Total Loss: {avg_losses['total_loss']:.4f}, \"\n",
    "                      f\"Recon Loss: {avg_losses['reconstruction_loss']:.4f}, \"\n",
    "                      f\"KL Loss: {avg_losses['kl_loss']:.4f}\")\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def generate_formulas(self, n_samples=10, temperature=1.0):\n",
    "        \"\"\"\n",
    "        Generate công thức mới\n",
    "        \"\"\"\n",
    "        if self.decoder is None:\n",
    "            raise ValueError(\"Model chưa được train!\")\n",
    "        \n",
    "        # Sample từ latent space\n",
    "        z_samples = np.random.normal(0, 1, (n_samples, self.latent_dim))\n",
    "        \n",
    "        # Decode\n",
    "        generated_sequences = self.decoder.predict(z_samples, verbose=0)\n",
    "        \n",
    "        # Convert về công thức\n",
    "        formulas = []\n",
    "        for seq in generated_sequences:\n",
    "            # Sample với temperature\n",
    "            if temperature != 1.0:\n",
    "                seq = seq / temperature\n",
    "                seq = tf.nn.softmax(seq, axis=-1).numpy()\n",
    "            \n",
    "            # Greedy decoding\n",
    "            tokens = np.argmax(seq, axis=-1)\n",
    "            \n",
    "            # Convert tokens về text\n",
    "            try:\n",
    "                formula_tokens = []\n",
    "                for token_id in tokens:\n",
    "                    if token_id == 0:  # Padding token\n",
    "                        break\n",
    "                    if token_id in self.tokenizer.index_word:\n",
    "                        formula_tokens.append(self.tokenizer.index_word[token_id])\n",
    "                \n",
    "                # Ghép thành công thức\n",
    "                formula = ''.join(formula_tokens)\n",
    "                if formula and self.validate_formula(formula):\n",
    "                    formulas.append(formula)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        return formulas\n",
    "    \n",
    "    def interpolate_formulas(self, formula1, formula2, n_steps=5):\n",
    "        \"\"\"\n",
    "        Interpolate giữa 2 công thức trong latent space\n",
    "        \"\"\"\n",
    "        # Encode 2 công thức\n",
    "        seq1 = self.prepare_single_formula(formula1)\n",
    "        seq2 = self.prepare_single_formula(formula2)\n",
    "        \n",
    "        z1_mean, z1_log_var = self.encoder.predict(seq1.reshape(1, -1), verbose=0)\n",
    "        z2_mean, z2_log_var = self.encoder.predict(seq2.reshape(1, -1), verbose=0)\n",
    "        \n",
    "        # Use mean values for interpolation\n",
    "        interpolated_formulas = []\n",
    "        for i in range(n_steps):\n",
    "            alpha = i / (n_steps - 1) if n_steps > 1 else 0\n",
    "            z_interp = (1 - alpha) * z1_mean + alpha * z2_mean\n",
    "            \n",
    "            # Decode\n",
    "            generated_seq = self.decoder.predict(z_interp, verbose=0)\n",
    "            tokens = np.argmax(generated_seq[0], axis=-1)\n",
    "            \n",
    "            # Convert về công thức\n",
    "            formula_tokens = []\n",
    "            for token_id in tokens:\n",
    "                if token_id == 0:\n",
    "                    break\n",
    "                if token_id in self.tokenizer.index_word:\n",
    "                    formula_tokens.append(self.tokenizer.index_word[token_id])\n",
    "            \n",
    "            formula = ''.join(formula_tokens)\n",
    "            if formula and self.validate_formula(formula):\n",
    "                interpolated_formulas.append(formula)\n",
    "        \n",
    "        return interpolated_formulas\n",
    "    \n",
    "    def prepare_single_formula(self, formula):\n",
    "        \"\"\"\n",
    "        Chuẩn bị 1 công thức để encode\n",
    "        \"\"\"\n",
    "        tokens = self.tokenize_formula(formula)\n",
    "        tokenized = ' '.join(tokens)\n",
    "        sequence = self.tokenizer.texts_to_sequences([tokenized])\n",
    "        padded = pad_sequences(sequence, maxlen=self.max_length, padding='post')\n",
    "        return padded[0]\n",
    "    \n",
    "    def validate_formula(self, formula):\n",
    "        \"\"\"\n",
    "        Kiểm tra tính hợp lệ cơ bản của công thức\n",
    "        \"\"\"\n",
    "        # Kiểm tra có chứa ít nhất 1 element\n",
    "        if not re.search(r'[A-Z]', formula):\n",
    "            return False\n",
    "        \n",
    "        # Kiểm tra format cơ bản\n",
    "        if not re.match(r'^([A-Z][a-z]*\\d*)+$', formula):\n",
    "            return False\n",
    "        \n",
    "        # Kiểm tra không có số 0\n",
    "        if '0' in formula:\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "\n",
    "# Ví dụ sử dụng\n",
    "if __name__ == \"__main__\":\n",
    "    data = pd.read_csv(\"/Users/sonn/Sonn/Workspace/Projects/IonBatteryQML/data/CrystalLithiumIonBattery.csv\")\n",
    "    sample_formulas = data['Formula']\n",
    "    \n",
    "    # Tạo và train model\n",
    "    vae = ChemicalFormulaVAE(latent_dim=32, max_length=15)\n",
    "    \n",
    "    # Chuẩn bị dữ liệu\n",
    "    sequences = vae.prepare_data(sample_formulas)\n",
    "    print(f\"Prepared {len(sequences)} sequences\")\n",
    "    \n",
    "    # Build models\n",
    "    vae.build_models()\n",
    "    print(\"\\nModel built successfully!\")\n",
    "    print(\"Encoder summary:\")\n",
    "    vae.encoder.summary()\n",
    "    print(\"\\nDecoder summary:\")\n",
    "    vae.decoder.summary()\n",
    "    \n",
    "    # Train\n",
    "    print(\"\\nTraining model...\")\n",
    "    history = vae.train(sequences, epochs=50, batch_size=8)\n",
    "    \n",
    "    # Generate công thức mới\n",
    "    print(\"\\nGenerating new formulas:\")\n",
    "    new_formulas = vae.generate_formulas(n_samples=20, temperature=0.8)\n",
    "    \n",
    "    print(f\"\\nOriginal formulas ({len(sample_formulas)}):\")\n",
    "    for f in sample_formulas:\n",
    "        print(f\"  {f}\")\n",
    "    \n",
    "    print(f\"\\nGenerated valid formulas ({len(new_formulas)}):\")\n",
    "    for f in new_formulas:\n",
    "        print(f\"  {f}\")\n",
    "    \n",
    "    # Interpolation example\n",
    "    if len(sample_formulas) >= 2:\n",
    "        print(f\"\\nInterpolation between {sample_formulas[0]} and {sample_formulas[1]}:\")\n",
    "        interpolated = vae.interpolate_formulas(sample_formulas[0], sample_formulas[1], n_steps=5)\n",
    "        for i, f in enumerate(interpolated):\n",
    "            print(f\"  Step {i}: {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "073e2068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "class ChemicalFormulaVAE:\n",
    "    def __init__(self, latent_dim=64, max_length=20):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.max_length = max_length + 2  # +2 cho <START> và <END>\n",
    "        self.tokenizer = None\n",
    "        self.vocab_size = None\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "    def tokenize_formula(self, formula):\n",
    "        pattern = r'([A-Z][a-z]*)(\\d*)'\n",
    "        tokens = []\n",
    "        matches = re.findall(pattern, formula)\n",
    "        for element, number in matches:\n",
    "            tokens.append(element)\n",
    "            if number:\n",
    "                tokens.append(number)\n",
    "        return tokens\n",
    "\n",
    "    def prepare_data(self, formulas):\n",
    "        tokenized_formulas = []\n",
    "        for formula in formulas:\n",
    "            tokens = self.tokenize_formula(formula)\n",
    "            tokenized = '<START> ' + ' '.join(tokens) + ' <END>'\n",
    "            tokenized_formulas.append(tokenized)\n",
    "\n",
    "        #self.tokenizer = Tokenizer(char_level=False, oov_token='<OOV>')\n",
    "        self.tokenizer = Tokenizer(char_level=False, lower=False, oov_token='<OOV>')\n",
    "        self.tokenizer.fit_on_texts(tokenized_formulas)\n",
    "        self.vocab_size = len(self.tokenizer.word_index) + 1\n",
    "\n",
    "        sequences = self.tokenizer.texts_to_sequences(tokenized_formulas)\n",
    "        padded = pad_sequences(sequences, maxlen=self.max_length, padding='post')\n",
    "\n",
    "        return padded\n",
    "\n",
    "    def build_encoder(self):\n",
    "        inputs = Input(shape=(self.max_length - 1,))  # <-- Sửa ở đây\n",
    "        x = Embedding(self.vocab_size, 128, mask_zero=True)(inputs)\n",
    "        x = LSTM(256, return_sequences=True)(x)\n",
    "        x = LSTM(128)(x)\n",
    "        z_mean = Dense(self.latent_dim, name='z_mean')(x)\n",
    "        z_log_var = Dense(self.latent_dim, name='z_log_var')(x)\n",
    "        return Model(inputs, [z_mean, z_log_var], name='encoder')\n",
    "\n",
    "    def build_decoder(self):\n",
    "        latent_inputs = Input(shape=(self.latent_dim,))\n",
    "        x = Dense(128, activation='relu')(latent_inputs)\n",
    "        x = Dense(256, activation='relu')(x)\n",
    "        x = RepeatVector(self.max_length - 1)(x)\n",
    "        x = LSTM(256, return_sequences=True)(x)\n",
    "        x = LSTM(128, return_sequences=True)(x)\n",
    "        outputs = Dense(self.vocab_size, activation='softmax')(x)\n",
    "        return Model(latent_inputs, outputs, name='decoder')\n",
    "\n",
    "    def sampling(self, z_mean, z_log_var):\n",
    "        epsilon = tf.random.normal(shape=tf.shape(z_mean))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "    def build_models(self):\n",
    "        self.encoder = self.build_encoder()\n",
    "        self.decoder = self.build_decoder()\n",
    "\n",
    "    def train_step(self, encoder_input, decoder_target):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var = self.encoder(encoder_input)\n",
    "            z = self.sampling(z_mean, z_log_var)\n",
    "            reconstruction = self.decoder(z)\n",
    "\n",
    "            loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "            reconstruction_loss = loss_fn(decoder_target, reconstruction)\n",
    "            reconstruction_loss = tf.reduce_mean(reconstruction_loss)\n",
    "\n",
    "            kl_loss = -0.5 * tf.reduce_mean(\n",
    "                1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
    "            )\n",
    "            total_loss = reconstruction_loss + 0.1 * kl_loss\n",
    "\n",
    "        grads = tape.gradient(total_loss, self.encoder.trainable_variables + self.decoder.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.encoder.trainable_variables + self.decoder.trainable_variables))\n",
    "\n",
    "        return {\n",
    "            'total_loss': total_loss,\n",
    "            'reconstruction_loss': reconstruction_loss,\n",
    "            'kl_loss': kl_loss\n",
    "        }\n",
    "\n",
    "    def train(self, sequences, epochs=50, batch_size=32):\n",
    "        decoder_input = sequences[:, :-1]\n",
    "        decoder_target = sequences[:, 1:]\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((decoder_input, decoder_target))\n",
    "        dataset = dataset.shuffle(1000).batch(batch_size)\n",
    "\n",
    "        history = {'total_loss': [], 'reconstruction_loss': [], 'kl_loss': []}\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = {'total_loss': [], 'reconstruction_loss': [], 'kl_loss': []}\n",
    "            for x_batch, y_batch in dataset:\n",
    "                losses = self.train_step(x_batch, y_batch)\n",
    "                for k in losses:\n",
    "                    epoch_loss[k].append(losses[k])\n",
    "\n",
    "            for k in epoch_loss:\n",
    "                history[k].append(np.mean(epoch_loss[k]))\n",
    "\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs} - \"\n",
    "                      f\"Loss: {history['total_loss'][-1]:.4f}, \"\n",
    "                      f\"Recon: {history['reconstruction_loss'][-1]:.4f}, \"\n",
    "                      f\"KL: {history['kl_loss'][-1]:.4f}\")\n",
    "        return history\n",
    "\n",
    "    # def generate_formulas(self, n_samples=10, temperature=1.0):\n",
    "    #     z_samples = np.random.normal(0, 1, (n_samples, self.latent_dim))\n",
    "    #     predictions = self.decoder.predict(z_samples, verbose=0)\n",
    "    #     formulas = []\n",
    "\n",
    "    #     for seq in predictions:\n",
    "    #         if temperature != 1.0:\n",
    "    #             seq = seq / temperature\n",
    "    #             seq = tf.nn.softmax(seq, axis=-1).numpy()\n",
    "\n",
    "    #         token_ids = np.argmax(seq, axis=-1)\n",
    "    #         tokens = []\n",
    "    #         for token_id in token_ids:\n",
    "    #             word = self.tokenizer.index_word.get(token_id)\n",
    "    #             if word == '<END>':\n",
    "    #                 break\n",
    "    #             if word and word not in ['<START>', '<OOV>']:\n",
    "    #                 tokens.append(word)\n",
    "    #         formula = ''.join(tokens)\n",
    "    #         if self.validate_formula(formula):\n",
    "    #             formulas.append(formula)\n",
    "    #     return formulas\n",
    "    # def generate_formulas(self, n_samples=10, temperature=1.0):\n",
    "    #     if self.decoder is None:\n",
    "    #         raise ValueError(\"Model chưa được train!\")\n",
    "\n",
    "    #     # Sample z từ latent space\n",
    "    #     z_samples = np.random.normal(0, 1, (n_samples, self.latent_dim))\n",
    "    #     predictions = self.decoder.predict(z_samples, verbose=0)\n",
    "\n",
    "    #     formulas = []\n",
    "\n",
    "    #     for seq in predictions:\n",
    "    #         # Temperature scaling\n",
    "    #         if temperature != 1.0:\n",
    "    #             seq = seq / temperature\n",
    "    #             seq = tf.nn.softmax(seq, axis=-1).numpy()\n",
    "\n",
    "    #         token_ids = np.argmax(seq, axis=-1)\n",
    "    #         tokens = []\n",
    "    #         for token_id in token_ids:\n",
    "    #             word = self.tokenizer.index_word.get(token_id)\n",
    "    #             if word == '<END>':\n",
    "    #                 break\n",
    "    #             if word and word not in ['<START>', '<OOV>']:\n",
    "    #                 tokens.append(word)\n",
    "    #         formula = ''.join(tokens)\n",
    "    #         if self.validate_formula(formula):\n",
    "    #             formulas.append(formula)\n",
    "\n",
    "    #     return formulas\n",
    "\n",
    "    def generate_formulas(self, n_samples=10, temperature=1.0):\n",
    "        if self.decoder is None:\n",
    "            raise ValueError(\"Model chưa được train!\")\n",
    "\n",
    "        z_samples = np.random.normal(0, 1, (n_samples, self.latent_dim))\n",
    "        sequences = self.decoder.predict(z_samples, verbose=0)\n",
    "\n",
    "        formulas = []\n",
    "\n",
    "        for i, seq in enumerate(sequences):\n",
    "            if temperature != 1.0:\n",
    "                seq = seq / temperature\n",
    "                seq = tf.nn.softmax(seq, axis=-1).numpy()\n",
    "\n",
    "            token_ids = np.argmax(seq, axis=-1)\n",
    "            tokens = []\n",
    "            for token_id in token_ids:\n",
    "                word = self.tokenizer.index_word.get(token_id, None)\n",
    "                if word == '<END>':\n",
    "                    break\n",
    "                if word and word not in ['<START>', '<OOV>']:\n",
    "                    tokens.append(word)\n",
    "\n",
    "            formula = ''.join(tokens)\n",
    "            print(f\"[DEBUG] Sample {i}: token_ids = {token_ids}\")\n",
    "            print(f\"[DEBUG] Sample {i}: raw tokens = {tokens}\")\n",
    "            print(f\"[DEBUG] Sample {i}: joined formula = {formula}\")\n",
    "\n",
    "            if self.validate_formula(formula):\n",
    "                formulas.append(formula)\n",
    "\n",
    "        return formulas\n",
    "\n",
    "\n",
    "\n",
    "    # def interpolate_formulas(self, formula1, formula2, n_steps=5):\n",
    "    #     seq1 = self.prepare_single_formula(formula1)\n",
    "    #     seq2 = self.prepare_single_formula(formula2)\n",
    "\n",
    "    #     z1_mean, _ = self.encoder.predict(seq1[np.newaxis, :])\n",
    "    #     z2_mean, _ = self.encoder.predict(seq2[np.newaxis, :])\n",
    "\n",
    "    #     results = []\n",
    "    #     for alpha in np.linspace(0, 1, n_steps):\n",
    "    #         z = (1 - alpha) * z1_mean + alpha * z2_mean\n",
    "    #         pred = self.decoder.predict(z, verbose=0)\n",
    "    #         token_ids = np.argmax(pred[0], axis=-1)\n",
    "    #         tokens = []\n",
    "    #         for token_id in token_ids:\n",
    "    #             word = self.tokenizer.index_word.get(token_id)\n",
    "    #             if word == '<END>':\n",
    "    #                 break\n",
    "    #             if word and word not in ['<START>', '<OOV>']:\n",
    "    #                 tokens.append(word)\n",
    "    #         formula = ''.join(tokens)\n",
    "    #         if self.validate_formula(formula):\n",
    "    #             results.append(formula)\n",
    "    #     return results\n",
    "    def interpolate_formulas(self, formula1, formula2, n_steps=5):\n",
    "        # Chuẩn bị 2 công thức đầu vào\n",
    "        seq1 = self.prepare_single_formula(formula1)\n",
    "        seq2 = self.prepare_single_formula(formula2)\n",
    "\n",
    "        # Lấy latent vectors (chỉ lấy mean, bỏ logvar)\n",
    "        z1_mean, _ = self.encoder.predict(seq1[np.newaxis, :], verbose=0)\n",
    "        z2_mean, _ = self.encoder.predict(seq2[np.newaxis, :], verbose=0)\n",
    "\n",
    "        # Tạo dãy vector nội suy\n",
    "        alphas = np.linspace(0, 1, n_steps)\n",
    "        z_interp = np.vstack([\n",
    "            (1 - alpha) * z1_mean + alpha * z2_mean for alpha in alphas\n",
    "        ])\n",
    "\n",
    "        preds = self.decoder.predict(z_interp, verbose=0)\n",
    "        results = []\n",
    "\n",
    "        for seq in preds:\n",
    "            token_ids = np.argmax(seq, axis=-1)\n",
    "            tokens = []\n",
    "            for token_id in token_ids:\n",
    "                word = self.tokenizer.index_word.get(token_id)\n",
    "                if word == '<END>':\n",
    "                    break\n",
    "                if word and word not in ['<START>', '<OOV>']:\n",
    "                    tokens.append(word)\n",
    "            formula = ''.join(tokens)\n",
    "            if self.validate_formula(formula):\n",
    "                results.append(formula)\n",
    "\n",
    "        return results\n",
    "        \n",
    "\n",
    "\n",
    "    # def prepare_single_formula(self, formula):\n",
    "    #     tokens = self.tokenize_formula(formula)\n",
    "    #     sequence = '<START> ' + ' '.join(tokens) + ' <END>'\n",
    "    #     seq = self.tokenizer.texts_to_sequences([sequence])\n",
    "    #     return pad_sequences(seq, maxlen=self.max_length, padding='post')[0]\n",
    "    def prepare_single_formula(self, formula):\n",
    "        tokens = self.tokenize_formula(formula)\n",
    "        sequence = '<START> ' + ' '.join(tokens) + ' <END>'\n",
    "        seq = self.tokenizer.texts_to_sequences([sequence])\n",
    "        padded = pad_sequences(seq, maxlen=self.max_length, padding='post')\n",
    "        return padded[0][:-1]  # <-- Trả về input giống lúc train encoder\n",
    "\n",
    "\n",
    "    def validate_formula(self, formula):\n",
    "        if not re.search(r'[A-Z]', formula): return False\n",
    "        if not re.match(r'^([A-Z][a-z]*\\d*)+$', formula): return False\n",
    "        # if '0' in formula: return False\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d01f929d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Training...\n",
      "Epoch 10/50 - Loss: 0.8214, Recon: 0.8107, KL: 0.1069\n",
      "Epoch 20/50 - Loss: 0.6203, Recon: 0.5951, KL: 0.2526\n",
      "Epoch 30/50 - Loss: 0.5243, Recon: 0.5019, KL: 0.2239\n",
      "Epoch 40/50 - Loss: 0.4896, Recon: 0.4584, KL: 0.3112\n",
      "Epoch 50/50 - Loss: 0.4631, Recon: 0.4354, KL: 0.2773\n",
      "\n",
      "Generated formulas:\n",
      "[DEBUG] Sample 0: token_ids = [ 3  7  9  7  4  5 12  6  0  0  0  0  0  0  0  0]\n",
      "[DEBUG] Sample 0: raw tokens = ['Li', '2', 'Fe', '2', 'Si', 'O', '5', 'END']\n",
      "[DEBUG] Sample 0: joined formula = Li2Fe2SiO5END\n",
      "[DEBUG] Sample 1: token_ids = [ 3  7  9  7  4  5 10  6  0  0  0  0  0  0  0  0]\n",
      "[DEBUG] Sample 1: raw tokens = ['Li', '2', 'Fe', '2', 'Si', 'O', '4', 'END']\n",
      "[DEBUG] Sample 1: joined formula = Li2Fe2SiO4END\n",
      "[DEBUG] Sample 2: token_ids = [3 7 9 4 7 5 6 6 0 0 0 0 0 0 0 0]\n",
      "[DEBUG] Sample 2: raw tokens = ['Li', '2', 'Fe', 'Si', '2', 'O', 'END', 'END']\n",
      "[DEBUG] Sample 2: joined formula = Li2FeSi2OENDEND\n",
      "[DEBUG] Sample 3: token_ids = [ 3  7  9  7  4  5 10  6  0  0  0  0  0  0  0  0]\n",
      "[DEBUG] Sample 3: raw tokens = ['Li', '2', 'Fe', '2', 'Si', 'O', '4', 'END']\n",
      "[DEBUG] Sample 3: joined formula = Li2Fe2SiO4END\n",
      "[DEBUG] Sample 4: token_ids = [ 3  7  9  7  4  5 12  6  0  0  0  0  0  0  0  0]\n",
      "[DEBUG] Sample 4: raw tokens = ['Li', '2', 'Fe', '2', 'Si', 'O', '5', 'END']\n",
      "[DEBUG] Sample 4: joined formula = Li2Fe2SiO5END\n",
      "[DEBUG] Sample 5: token_ids = [ 3  7  9  7  4  5 12  6  0  0  0  0  0  0  0  0]\n",
      "[DEBUG] Sample 5: raw tokens = ['Li', '2', 'Fe', '2', 'Si', 'O', '5', 'END']\n",
      "[DEBUG] Sample 5: joined formula = Li2Fe2SiO5END\n",
      "[DEBUG] Sample 6: token_ids = [3 7 9 4 5 8 6 0 0 0 0 0 0 0 0 0]\n",
      "[DEBUG] Sample 6: raw tokens = ['Li', '2', 'Fe', 'Si', 'O', '3', 'END']\n",
      "[DEBUG] Sample 6: joined formula = Li2FeSiO3END\n",
      "[DEBUG] Sample 7: token_ids = [ 3  7  9  4  5 10  6  0  0  0  0  0  0  0  0  0]\n",
      "[DEBUG] Sample 7: raw tokens = ['Li', '2', 'Fe', 'Si', 'O', '4', 'END']\n",
      "[DEBUG] Sample 7: joined formula = Li2FeSiO4END\n",
      "[DEBUG] Sample 8: token_ids = [ 3  7  9  7  4  5 10  6  0  0  0  0  0  0  0  0]\n",
      "[DEBUG] Sample 8: raw tokens = ['Li', '2', 'Fe', '2', 'Si', 'O', '4', 'END']\n",
      "[DEBUG] Sample 8: joined formula = Li2Fe2SiO4END\n",
      "[DEBUG] Sample 9: token_ids = [ 3  7  9  4  4  5 12  6  0  0  0  0  0  0  0  0]\n",
      "[DEBUG] Sample 9: raw tokens = ['Li', '2', 'Fe', 'Si', 'Si', 'O', '5', 'END']\n",
      "[DEBUG] Sample 9: joined formula = Li2FeSiSiO5END\n",
      "Li2Fe2SiO5END\n",
      "Li2Fe2SiO4END\n",
      "Li2FeSi2OENDEND\n",
      "Li2Fe2SiO4END\n",
      "Li2Fe2SiO5END\n",
      "Li2Fe2SiO5END\n",
      "Li2FeSiO3END\n",
      "Li2FeSiO4END\n",
      "Li2Fe2SiO4END\n",
      "Li2FeSiSiO5END\n",
      "\n",
      "Interpolation example:\n",
      "Between Li2MnSiO4 and Li4MnSi2O7:\n",
      "  Step 0: Li2FeSiO4END\n",
      "  Step 1: Li2FeSiOOEND\n",
      "  Step 2: Li2FeSiSiO5END\n",
      "  Step 3: Li2Fe2SiO5END\n",
      "  Step 4: Li2Fe2SiO5END\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    data = pd.read_csv(\"/Users/sonn/Sonn/Workspace/Projects/IonBatteryQML/data/CrystalLithiumIonBattery.csv\")\n",
    "    sample_formulas = data['Formula'].dropna().unique()\n",
    "\n",
    "    vae = ChemicalFormulaVAE(latent_dim=32, max_length=15)\n",
    "    sequences = vae.prepare_data(sample_formulas)\n",
    "\n",
    "    print(\"Building model...\")\n",
    "    vae.build_models()\n",
    "\n",
    "    print(\"Training...\")\n",
    "    vae.train(sequences, epochs=50, batch_size=8)\n",
    "\n",
    "\n",
    "    print(\"\\nGenerated formulas:\")\n",
    "    new_formulas = vae.generate_formulas(n_samples=10, temperature=0.6)\n",
    "    for f in new_formulas:\n",
    "        print(f)\n",
    "\n",
    "    print(\"\\nInterpolation example:\")\n",
    "    if len(sample_formulas) >= 2:\n",
    "        f1, f2 = sample_formulas[0], sample_formulas[1]\n",
    "        print(f\"Between {f1} and {f2}:\")\n",
    "        inter = vae.interpolate_formulas(f1, f2, n_steps=5)\n",
    "        for i, f in enumerate(inter):\n",
    "            print(f\"  Step {i}: {f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dd8db614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Materials Id</th>\n",
       "      <th>Formula</th>\n",
       "      <th>Spacegroup</th>\n",
       "      <th>Formation Energy (eV)</th>\n",
       "      <th>E Above Hull (eV)</th>\n",
       "      <th>Band Gap (eV)</th>\n",
       "      <th>Nsites</th>\n",
       "      <th>Density (gm/cc)</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Has Bandstructure</th>\n",
       "      <th>Crystal System</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mp-849394</td>\n",
       "      <td>Li2MnSiO4</td>\n",
       "      <td>Pc</td>\n",
       "      <td>-2.699</td>\n",
       "      <td>0.006</td>\n",
       "      <td>3.462</td>\n",
       "      <td>16</td>\n",
       "      <td>2.993</td>\n",
       "      <td>178.513</td>\n",
       "      <td>True</td>\n",
       "      <td>monoclinic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mp-783909</td>\n",
       "      <td>Li2MnSiO4</td>\n",
       "      <td>P21/c</td>\n",
       "      <td>-2.696</td>\n",
       "      <td>0.008</td>\n",
       "      <td>2.879</td>\n",
       "      <td>32</td>\n",
       "      <td>2.926</td>\n",
       "      <td>365.272</td>\n",
       "      <td>True</td>\n",
       "      <td>monoclinic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mp-761311</td>\n",
       "      <td>Li4MnSi2O7</td>\n",
       "      <td>Cc</td>\n",
       "      <td>-2.775</td>\n",
       "      <td>0.012</td>\n",
       "      <td>3.653</td>\n",
       "      <td>28</td>\n",
       "      <td>2.761</td>\n",
       "      <td>301.775</td>\n",
       "      <td>True</td>\n",
       "      <td>monoclinic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mp-761598</td>\n",
       "      <td>Li4Mn2Si3O10</td>\n",
       "      <td>C2/c</td>\n",
       "      <td>-2.783</td>\n",
       "      <td>0.013</td>\n",
       "      <td>3.015</td>\n",
       "      <td>38</td>\n",
       "      <td>2.908</td>\n",
       "      <td>436.183</td>\n",
       "      <td>True</td>\n",
       "      <td>monoclinic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mp-767709</td>\n",
       "      <td>Li2Mn3Si3O10</td>\n",
       "      <td>C2/c</td>\n",
       "      <td>-2.747</td>\n",
       "      <td>0.016</td>\n",
       "      <td>2.578</td>\n",
       "      <td>36</td>\n",
       "      <td>3.334</td>\n",
       "      <td>421.286</td>\n",
       "      <td>True</td>\n",
       "      <td>monoclinic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Materials Id       Formula Spacegroup  Formation Energy (eV)  \\\n",
       "0    mp-849394     Li2MnSiO4         Pc                 -2.699   \n",
       "1    mp-783909     Li2MnSiO4      P21/c                 -2.696   \n",
       "2    mp-761311    Li4MnSi2O7         Cc                 -2.775   \n",
       "3    mp-761598  Li4Mn2Si3O10       C2/c                 -2.783   \n",
       "4    mp-767709  Li2Mn3Si3O10       C2/c                 -2.747   \n",
       "\n",
       "   E Above Hull (eV)  Band Gap (eV)  Nsites  Density (gm/cc)   Volume  \\\n",
       "0              0.006          3.462      16            2.993  178.513   \n",
       "1              0.008          2.879      32            2.926  365.272   \n",
       "2              0.012          3.653      28            2.761  301.775   \n",
       "3              0.013          3.015      38            2.908  436.183   \n",
       "4              0.016          2.578      36            3.334  421.286   \n",
       "\n",
       "   Has Bandstructure Crystal System  \n",
       "0               True     monoclinic  \n",
       "1               True     monoclinic  \n",
       "2               True     monoclinic  \n",
       "3               True     monoclinic  \n",
       "4               True     monoclinic  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"/Users/sonn/Sonn/Workspace/Projects/IonBatteryQML/data/CrystalLithiumIonBattery.csv\")\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c9186026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token index for Mn: Not found\n",
      "Vocabulary size: 27\n",
      "All tokens: ['<OOV>', 'start', 'li', 'si', 'o', 'end', '2', '3', 'fe', '4', 'mn', '5', '7', 'co', '10', '8', '6', '16', '13', '11', '9', '15', '17', '19', '32', '24']\n"
     ]
    }
   ],
   "source": [
    "print(\"Token index for Mn:\", vae.tokenizer.word_index.get(\"Mn\", \"Not found\"))\n",
    "print(\"Vocabulary size:\", vae.vocab_size)\n",
    "print(\"All tokens:\", list(vae.tokenizer.word_index.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ac1de5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Li', '2', 'Mn', 'Si', 'O', '4']\n"
     ]
    }
   ],
   "source": [
    "print(vae.tokenize_formula(\"Li2MnSiO4\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WSAIPytorch3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
